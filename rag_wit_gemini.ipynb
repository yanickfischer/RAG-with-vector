{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Agent with Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os  # For interacting with the operating system, e.g., file paths\n",
    "import asyncio  # For managing asynchronous tasks\n",
    "\n",
    "# Third-party library imports\n",
    "from dotenv import load_dotenv  # For loading environment variables from a .env file\n",
    "from PyPDF2 import PdfReader  # For reading PDF files\n",
    "import tqdm  # For displaying progress bars in loops\n",
    "\n",
    "# LangChain imports - Core functionality\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting text into manageable chunks\n",
    "from langchain.prompts import PromptTemplate  # For defining and managing prompt templates\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain  # For combining retrieved documents into a coherent chain\n",
    "from langchain.globals import set_debug  # For enabling debug mode in LangChain\n",
    "\n",
    "# LangChain - Google Generative AI integrations\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # For generating embeddings using Google Generative AI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI  # For chat-based interactions with Google Generative AI\n",
    "\n",
    "# LangChain - Vector store\n",
    "from langchain_community.vectorstores import FAISS  # For storing and retrieving embeddings using the FAISS library\n",
    "\n",
    "# LangChain - Advanced prompt management and messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  # For creating structured chat prompts\n",
    "from langchain_core.messages import HumanMessage, AIMessage  # For handling human and AI messages\n",
    "from langchain_core.output_parsers import StrOutputParser  # For parsing string outputs from models\n",
    "from langchain_core.runnables import RunnableBranch  # For creating branches in the chain of execution\n",
    "\n",
    "from typing import Dict\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "import nest_asyncio\n",
    "import weave\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setting Up Environment Variables and PDF Path\n",
    "\n",
    "In this section, we:\n",
    "1. **Load Environment Variables**: We use the `load_dotenv()` function to load key-value pairs from a `.env` file into the environment. This allows us to securely manage sensitive information such as API keys.\n",
    "   - The API key for Google Generative AI is stored in an environment variable called `GOOGLE_API_KEY`.\n",
    "2. **Define the PDF Path**: The `pdf_path` variable specifies the location of the PDF file that we will process in subsequent steps.\n",
    "   - Ensure that the file exists at the specified path before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "pdf_path = \"data/nihms-1901028.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Reading and Extracting Text from the PDF\n",
    "\n",
    "In this step, we process the PDF file to extract its textual content.\n",
    "\n",
    "1. **Open the PDF File**\n",
    " \n",
    "2. **Initialize the PDF Reader**: We use the `PdfReader` class from the `PyPDF2` library to parse the PDF.\n",
    "\n",
    "3. **Extract Text**: \n",
    "   - A generator expression iterates over all pages in the PDF, using `page.extract_text()` to extract the text content of each page.\n",
    "   - Pages that do not contain text are skipped (`if page.extract_text()`).\n",
    "\n",
    "4. **Combine Text**: The extracted text from all pages is concatenated into a single string using `\"\".join(...)`.\n",
    "\n",
    "#### Notes:\n",
    "- If the PDF is large, this approach might consume significant memory. For large PDFs, consider processing pages in smaller batches.\n",
    "- The output variable `text` contains all the text extracted from the PDF and will be used in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pdf_path, \"rb\") as file:\n",
    "    reader = PdfReader(file)\n",
    "     # Extract text from all pages in the PDF\n",
    "    text = \"\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "    \n",
    "# Display the extracted text\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Splitting Text into Manageable Chunks\n",
    "\n",
    "In this step, we divide the extracted text into smaller, overlapping chunks for better processing in later stages.\n",
    "\n",
    "1. **Initialize the Text Splitter**:\n",
    "   - We use the `RecursiveCharacterTextSplitter` from LangChain, which is designed to split large texts into smaller pieces without necessarily respecting semantic coherence.\n",
    "   - Parameters:\n",
    "     - `chunk_size=10000`: Each chunk contains up to 10,000 characters.\n",
    "     - `chunk_overlap=1000`: Adjacent chunks overlap by 1,000 characters. This overlap ensures context is maintained across chunks.\n",
    "\n",
    "3. **Split the Text**:\n",
    "   - The `split_text()` method splits the input text (from the previous step) into chunks based on the specified parameters.\n",
    "   - The resulting `chunks` is a list of strings, each representing a section of the original text.\n",
    "\n",
    "#### Notes:\n",
    "- The choice of `chunk_size` and `chunk_overlap` depends on the use case and model constraints. Larger models can typically handle larger chunks.\n",
    "- The `chunks` will be used in downstream tasks such as retrieval or generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a text splitter with specified chunk size and overlap\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "# Split the extracted text into manageable chunks\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# Display the resulting chunks\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green; font-size: 16px;\">\n",
    "<b>Exercise: Experimenting with Text Chunking Parameters</b><br>\n",
    "<b>Goal:</b> Develop an understanding of how chunking parameters affect text segmentation and its impact on downstream tasks.<br><br>\n",
    "\n",
    "<b>Instructions:</b><br>\n",
    "<ul style=\"color:green;\">\n",
    "<li>After completing <b>Step 4: Splitting Text into Manageable Chunks (Cell 4)</b>, adjust the <code>chunk_size</code> and <code>chunk_overlap</code> parameters.</li>\n",
    "<li>Observe how these adjustments influence the number and size of the resulting chunks.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Purpose:</b> This exercise helps you build intuition about text chunking processes, enabling you to see how parameter choices can affect tasks that depend on text segmentation.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Creating and Saving a Vector Store\n",
    "\n",
    "In this step, we generate embeddings for the text chunks and store them in a vector database for efficient retrieval.\n",
    "\n",
    "1. **Generate Embeddings**:\n",
    "   - We use `GoogleGenerativeAIEmbeddings` to create embeddings for each text chunk.\n",
    "   - The parameter `model=\"models/embedding-001\"` specifies the embedding model to use. Ensure this model is available and properly configured in your environment.\n",
    "\n",
    "2. **Create the FAISS Vector Store**:\n",
    "   - FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "3. **Save the Vector Store**:\n",
    "   - The `save_local(\"faiss_index\")` method saves the FAISS index to a local file.\n",
    "   - This allows us to reuse the index in later sessions without re-processing the text or regenerating embeddings.\n",
    "\n",
    "#### Notes:\n",
    "- **Why Use FAISS?**\n",
    "  - It is highly optimized for large-scale vector searches and enables quick retrieval of relevant chunks for a given query.\n",
    "- **Next Steps**:\n",
    "  - The stored vector database will be used to retrieve the most relevant chunks of text when querying the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "vector_store.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Initializing Weave for Project Tracking\n",
    "\n",
    "In this step, we initialize **Weave**, a library designed for tracking and visualizing machine learning workflows and data flows.\n",
    "\n",
    "1. **What is Weave?**\n",
    "   - Weave is a tool for logging, monitoring, and debugging machine learning experiments and pipelines.\n",
    "   - It allows you to visualize your project’s structure, metrics, and progress, which is especially useful in iterative development.\n",
    "\n",
    "2. **Initialization**:\n",
    "   - The `weave.init()` function initializes a new Weave project. The argument `\"medical-data-chatbot\"` specifies the project name.\n",
    "   - This name will help organize and track this specific project in the Weave dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(\"medical-data-chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Setting Up the Retriever\n",
    "\n",
    "In this step, we configure a **retriever**.\n",
    "\n",
    "**Create a Retriever**:\n",
    "   - The `vector_store.as_retriever()` method converts the FAISS vector store into a retriever object.\n",
    "   - The parameter `k=4` specifies the maximum number of chunks to retrieve for each query. This ensures that only the top 4 most relevant chunks are returned.\n",
    "\n",
    "#### Notes:\n",
    "- **Why Limit the Results?**\n",
    "  - Limiting the number of results ensures that the model processes only the most relevant information, which can improve efficiency and response quality.\n",
    "  - The value of `k` can be adjusted based on the complexity of the query and the size of the text chunks.\n",
    "\n",
    "- **Next Steps**:\n",
    "  - The retriever will be used in conjunction with a generative model to form a **retrieval-augmented generation (RAG)** pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green; font-size: 16px;\">\n",
    "<b>Exercise: Exploring Retriever Functionality</b><br>\n",
    "<b>Goal:</b> Gain an understanding of how a retriever operates and how it determines document relevance.<br><br>\n",
    "\n",
    "<b>Instructions:</b><br>\n",
    "<ul style=\"color:green;\">\n",
    "<li>After completing <b>Step 7: Setting Up the Retriever (Cell 7)</b>, test the retriever with three distinct queries:</li>\n",
    "<ul style=\"color:green;\">\n",
    "    <li>A query closely related to the dataset content (relevant).</li>\n",
    "    <li>A query with ambiguous wording (vague).</li>\n",
    "    <li>A query entirely unrelated to the dataset (unrelated).</li>\n",
    "</ul>\n",
    "<li>For each query, analyze the documents retrieved and discuss the following:</li>\n",
    "<ul style=\"color:green;\">\n",
    "    <li>Why were these specific documents selected?</li>\n",
    "    <li>How well do the retrieved documents align with the query intent?</li>\n",
    "    <li>What patterns or limitations do you observe in the retriever's behavior?</li>\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "<b>Purpose:</b> This exercise helps you understand the principles and limitations of similarity-based retrieval, fostering insight into its performance across different types of queries.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Demonstrating the Retriever in Action\n",
    "\n",
    "In this step, we test the retriever by providing a sample query and observing the returned results.\n",
    "\n",
    "1. **Purpose of this Demonstration**:\n",
    "   - This step shows that the retriever functions independently and (correctly) fetches the most relevant chunks of text based on the query.\n",
    "\n",
    "2. **Query the Retriever**:\n",
    "   - The `retriever.invoke()` method takes a query (in this case, `\"What is the difference between high and medium protein-based diets?\"`) and searches the vector store for the most relevant chunks.\n",
    "   - The retriever returns the top `k=4` results, as configured earlier.\n",
    "\n",
    "3. **Output**:\n",
    "   - The `docs` variable contains the retrieved chunks as a list of text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"What is the difference between high and medium protein-based diets?\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Creating the Question-Answering System\n",
    "\n",
    "In this step, we set up the components needed to answer user questions based on the retrieved documents.\n",
    "\n",
    "1. **Define the System Template**:\n",
    "   - The `system_template` string specifies how the generative model should process the retrieved context.\n",
    "   - It instructs the model to answer the user's question using the information provided in the `<context>` placeholder.\n",
    "\n",
    "2. **Create a Prompt Template**:\n",
    "   - The `PromptTemplate` wraps the `system_template` into a reusable object.\n",
    "   - The `input_variables=[\"context\"]` defines which variables need to be filled in when the prompt is used.\n",
    "\n",
    "3. **Initialize the Generative Model**:\n",
    "   - The `ChatGoogleGenerativeAI` class is used to instantiate a chat-based model.\n",
    "   - Parameters:\n",
    "     - `model=\"gemini-1.5-pro-latest\"` specifies the version of the model to use.\n",
    "     - `temperature=0.5` controls the randomness of the responses. A value of `0.5` balances creativity and determinism.\n",
    "\n",
    "4. **Create the Document Chain**:\n",
    "   - The `create_stuff_documents_chain()` function integrates the model and the prompt into a chain.\n",
    "\n",
    "#### Notes:\n",
    "- **Model Selection**:\n",
    "   - The `\"gemini-1.5-pro-latest\"` model is used here, but it can be replaced with other compatible models if needed.\n",
    "- **Customizable Prompt**:\n",
    "   - The `system_template` can be adjusted to meet the requirements of different use cases\n",
    "- **Next Steps**:\n",
    "   - Use this document chain to generate answers for specific queries in combination with the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template for answering user questions based on a provided context\n",
    "system_template = \"\"\"\n",
    "Answer the users question based on the below context:\n",
    "<context> {context} </context>\n",
    "Say that you don't know the answer if you the context is not relevant to the question.\n",
    "\"\"\"\n",
    "# Create a prompt template for the question-answering system\n",
    "question_answering_prompt = PromptTemplate(template=system_template, input_variables=[\"context\"])\n",
    "\n",
    "# Initialize the generative model for question answering\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", temperature=0.5)\n",
    "\n",
    "# Create a document chain to handle the retrieval and response generation process\n",
    "document_chain = create_stuff_documents_chain(llm=model, prompt=question_answering_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Testing the Document Chain for Question-Answering\n",
    "\n",
    "In this step, we test the full retrieval-augmented generation (RAG) chain by invoking the `document_chain` with a user query and the retrieved context.\n",
    "\n",
    "1. **Purpose**:\n",
    "   - This step demonstrates how the `document_chain` integrates the retrieved context (`docs`) and the user query to generate a response using the generative model.\n",
    "\n",
    "2. **Components of the Input**:\n",
    "   - **Context**:\n",
    "     - The `context` key is assigned the value of `docs`, which contains the top chunks retrieved by the retriever in **Step 7**.\n",
    "     - This ensures the model has access to relevant information when answering the query.\n",
    "   - **Messages**:\n",
    "     - A list of messages simulates a conversational interaction.\n",
    "     - The `HumanMessage` object represents the user query: `\"What is the difference between high and medium protein-based diets?\"`.\n",
    "\n",
    "4. **Expected Output**:\n",
    "   - The output is a response from the model, which synthesizes the retrieved context and the query to provide an accurate and relevant answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain.invoke(\n",
    "    {\n",
    "        \"context\": docs,\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"What is the difference between high and medium protein-based diets?\")\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Building a Combined Retrieval Chain\n",
    "\n",
    "In this step, we create a combined retrieval-augmented generation (RAG) chain that integrates retrieval and document generation into a seamless pipeline.\n",
    "\n",
    "1. **Helper Function**:\n",
    "   - The `parse_retriever_input` function extracts the latest user query from the `params` dictionary.\n",
    "   - Specifically, it accesses the `\"messages\"` key and retrieves the content of the last message, which represents the most recent user query.\n",
    "\n",
    "2. **RunnablePassthrough**:\n",
    "   - A `RunnablePassthrough` is a utility that passes data through the specified processing steps without additional transformation.\n",
    "   - We use its `.assign()` method to define the sequence of operations in the chain.\n",
    "\n",
    "3. **Assigning Operations**:\n",
    "   - The chain is composed of two key steps:\n",
    "     - **Step 1: Retrieve Context**:\n",
    "       - The `parse_retriever_input` function extracts the user query.\n",
    "       - This query is passed through the `retriever` to fetch the relevant text chunks.\n",
    "       - The result is assigned to the `context` key.\n",
    "     - **Step 2: Generate Answer**:\n",
    "       - The `document_chain` takes the retrieved context and generates an answer based on the query.\n",
    "       - The result is assigned to the `answer` key.\n",
    "\n",
    "4. **Output**:\n",
    "   - The `retrieval_chain` object is now a runnable pipeline that combines retrieval and response generation in a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to extract the latest user query from the input parameters\n",
    "def parse_retriever_input(params: Dict):\n",
    "    return params[\"messages\"][-1].content\n",
    "\n",
    "# Create a retrieval chain with a passthrough mechanism\n",
    "retrieval_chain = RunnablePassthrough.assign(\n",
    "    # First step: Extract the user query and use it to retrieve relevant context\n",
    "    context=parse_retriever_input | retriever,\n",
    ").assign(\n",
    "    # Second step: Use the retrieved context to generate an answer\n",
    "    answer=document_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Testing the Combined Retrieval Chain\n",
    "\n",
    "In this step, we test the more complex `retrieval_chain` pipeline to ensure that it seamlessly integrates the retrieval and generation steps.\n",
    "\n",
    "1. **Purpose**:\n",
    "   - This test validates that the chain can process user queries end-to-end:\n",
    "     - Extracting the query.\n",
    "     - Retrieving the relevant context using the `retriever`.\n",
    "     - Generating a coherent response using the `document_chain`.\n",
    "\n",
    "2. **Input Structure**:\n",
    "   - The input is a dictionary with the key `\"messages\"`, which contains a list of messages.\n",
    "   - Each message is represented as a `HumanMessage` object.\n",
    "\n",
    "3. **Pipeline Execution**:\n",
    "   - **Query Extraction**: The `parse_retriever_input` function extracts the query from the last message in the list.\n",
    "   - **Context Retrieval**: The query is passed to the `retriever` to fetch the most relevant text chunks.\n",
    "   - **Answer Generation**: The retrieved context is fed into the `document_chain`, which uses the prompt and generative model to produce the final answer.\n",
    "\n",
    "4. **Expected Output**:\n",
    "   - A dictionary containing:\n",
    "     - `\"context\"`: The retrieved text chunks.\n",
    "     - `\"answer\"`: The generated response to the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"What is the difference between high and medium protein-based diets?\")\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Testing the Retrieval Chain with a Follow-Up Query\n",
    "\n",
    "This step demonstrates a limitation of the current retrieval pipeline when handling vague or follow-up queries without explicit reference to the context of the previous conversation.\n",
    "\n",
    "1. **Purpose**:\n",
    "   - To test how the retriever responds to a vague query such as `\"Tell me more\"`.\n",
    "   - Highlight the challenge of maintaining conversational context in the current implementation.\n",
    "\n",
    "2. **Current Behavior**:\n",
    "   - The `retrieval_chain` processes the input query `\"Tell me more\"` independently, without considering previous queries or their context.\n",
    "   - The retriever fetches documents that match the new query, but since `\"Tell me more\"` is nonspecific, the results may be irrelevant or nonsensical.\n",
    "\n",
    "3. **Expected Behavior**:\n",
    "   - Ideally, the system should infer that `\"Tell me more\"` is a continuation of the prior query (`\"What is the difference between high and medium protein-based diets?\"`).\n",
    "   - The retrieved documents should provide additional information about the initial topic.\n",
    "\n",
    "4. **Limitation**:\n",
    "   - The current design does not track conversational context or incorporate previous messages into the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Tell me more\")\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Testing the Retriever Directly with a Vague Query\n",
    "\n",
    "1. **Purpose**:\n",
    "   - To show that the retriever retrieves documents directly related to the query without any consideration of conversational history or prior context.\n",
    "   - This highlights the challenge of vague queries in isolation.\n",
    "\n",
    "2. **Current Behavior**:\n",
    "   - The retriever processes the query `\"Tell me more!\"` independently and returns documents that match this phrase based on the embedding similarity.\n",
    "   - Since `\"Tell me more!\"` lacks specific content, the results are likely to be generic or nonsensical unless a context happens to align by chance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"Tell me more!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 15: Adding Query Transformation to Improve Contextual Relevance\n",
    "\n",
    "This step introduces a **query transformation prompt** to address the limitations of vague queries like `\"Tell me more!\"`. The goal is to reframe such queries in the context of the conversation, producing a more meaningful query for the retriever.\n",
    "\n",
    "1. **Purpose**:\n",
    "   - To improve the retrieval system by generating a context-aware search query that reflects the ongoing conversation.\n",
    "   - This ensures that follow-up queries are relevant and meaningful, even if they are vague.\n",
    "\n",
    "2. **Query Transformation Prompt**:\n",
    "   - The `ChatPromptTemplate.from_messages()` method creates a prompt that uses all prior messages (`messages`) as context.\n",
    "   - The prompt asks the model to:\n",
    "     - Analyze the prior conversation.\n",
    "     - Generate a search query tailored to the user's intent and the ongoing context.\n",
    "     - Output **only** the transformed query for use with the retriever.\n",
    "\n",
    "#### Notes:\n",
    "- **Benefits**:\n",
    "   - This approach bridges the gap between conversational input and the retriever's expectations for specific queries.\n",
    "\n",
    "- **Limitations**:\n",
    "   - The effectiveness depends on the quality of the generative model used for query transformation.\n",
    "   - Ambiguous conversations might still produce suboptimal queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_transform_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green; font-size: 16px;\">\n",
    "<b>Exercise: Experimenting with Query Transformation</b><br>\n",
    "<b>Goal:</b> Understand the role of query transformation in improving contextual relevance and retrieval accuracy.<br><br>\n",
    "\n",
    "<b>Instructions:</b><br>\n",
    "<ul style=\"color:green;\">\n",
    "<li>After completing <b>Step 15: Adding Query Transformation to Improve Contextual Relevance (Cell 15)</b>, modify the <code>query_transform_prompt</code> to tailor the generated search queries for a specific domain, such as:</li>\n",
    "<ul style=\"color:green;\">\n",
    "    <li>Scientific research</li>\n",
    "    <li>Customer support</li>\n",
    "</ul>\n",
    "<li>Test the modified prompt by providing multiple follow-up queries, such as:</li>\n",
    "<ul style=\"color:green;\">\n",
    "    <li>\"Tell me more\"</li>\n",
    "    <li>\"Explain further\"</li>\n",
    "    <li>\"Can you provide examples?\"</li>\n",
    "</ul>\n",
    "<li>Observe and analyze the transformed queries for each example. Reflect on the following:</li>\n",
    "<ul style=\"color:green;\">\n",
    "    <li>How well do the transformed queries align with the domain-specific context?</li>\n",
    "    <li>Do the transformed queries improve retrieval accuracy for the intended domain?</li>\n",
    "    <li>What potential improvements could be made to the transformation prompt?</li>\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "<b>Purpose:</b> This exercise demonstrates how query transformation can enhance retrieval performance by aligning queries more closely with domain-specific needs, helping to fine-tune the system for specialized use cases.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Adding a Model to the Query Transformation Chain\n",
    "\n",
    "In this step, we enhance the query transformation process by integrating a generative model into the chain. This enables the system to dynamically generate refined, context-aware search queries.\n",
    "\n",
    "1. **Purpose**:\n",
    "   - To implement a **query transformation chain** that processes conversational context and user input to produce an optimized query for the retriever.\n",
    "   - By combining the transformation prompt with the generative model, we create an end-to-end pipeline for query reformulation.\n",
    "\n",
    "5. **Improved Retrieval**:\n",
    "   - The transformed query should be more specific and meaningful now, improving the retriever's ability to fetch relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_transformation_chain = query_transform_prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17: Testing the Query Transformation Chain\n",
    "\n",
    "In this step, we test the **query transformation chain** to verify its ability to refine vague queries like `\"Tell me more!\"` into meaningful search queries using the context of the preceding conversation.\n",
    "\n",
    "1. **Purpose**:\n",
    "   - To validate that the `query_transformation_chain` can analyze the conversational context and generate a search query relevant to the user’s intent.\n",
    "   - This test demonstrates how the chain integrates the conversational history when reformulating vague follow-up queries.\n",
    "\n",
    "2. **Input Structure**:\n",
    "   - **Messages**:\n",
    "     - A list of messages simulating a conversation:\n",
    "       - **First Message (Human)**: `\"What is the difference between high and medium protein-based diets?\"`\n",
    "       - **Second Message (AI)**: A detailed response summarizing research findings on protein-based diets.\n",
    "       - **Third Message (Human)**: `\"Tell me more!\"`, a vague follow-up query.\n",
    "   - The chain uses this conversation history to generate a refined query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_transformation_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"What is the difference between high and medium protein-based diets?\"),\n",
    "            AIMessage(\n",
    "                content=\"he study found that both high and normal protein diets improved body composition and glucose control in adults with type 2 diabetes. The lack of observed effects of dietary protein and red meat consumption on weight loss and improved cardiometabolic health suggest that achieved weight loss – rather than diet composition – should be the principal target of dietary interventions for T2D management.\"\n",
    "            ),\n",
    "            HumanMessage(content=\"Tell me more!\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 18: Building the Query-Transforming Retriever Chain\n",
    "\n",
    "In this step, we create a **query-transforming retriever chain** that dynamically adapts its behavior based on the structure of the input.\n",
    "\n",
    "1. **Purpose**:\n",
    "   - To handle two scenarios seamlessly:\n",
    "     - **Single Message**: When there is only one message, pass the content directly to the retriever.\n",
    "     - **Multiple Messages**: When there is a conversational history, use the query transformation chain to refine the query before passing it to the retriever.\n",
    "   - This flexible chain improves the system's ability to handle both straightforward and context-dependent queries.\n",
    "\n",
    "2. **Components**:\n",
    "   - **RunnableBranch**:\n",
    "     - Dynamically selects a branch to execute based on the input condition.\n",
    "   - **Condition**:\n",
    "     - The lambda function `lambda x: len(x.get(\"messages\", [])) == 1` checks if the input contains only one message.\n",
    "     - If `True`, the first branch is executed. Otherwise, the second branch is used.\n",
    "   - **First Branch**:\n",
    "     - If there is only one message:\n",
    "       - Extract the content of the last message with `lambda x: x[\"messages\"][-1].content`.\n",
    "       - Pass this content directly to the `retriever` to fetch relevant documents.\n",
    "   - **Second Branch**:\n",
    "     - If there are multiple messages:\n",
    "       - The input is passed through the query transformation pipeline:\n",
    "         1. `query_transform_prompt`: Captures and reformulates the query in context.\n",
    "         2. `model`: Generates the refined query.\n",
    "         3. `StrOutputParser()`: Parses the output string for compatibility with the retriever.\n",
    "       - The transformed query is passed to the `retriever`.\n",
    "\n",
    "3. **Configuration**:\n",
    "   - The `with_config(run_name=\"chat_retriever_chain\")` method assigns a unique name to this chain, making it easier to track during execution and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_transforming_retriever_chain = RunnableBranch(\n",
    "    (\n",
    "        lambda x: len(x.get(\"messages\", [])) == 1,\n",
    "        # If only one message, then we just pass that message's content to retriever\n",
    "        (lambda x: x[\"messages\"][-1].content) | retriever,\n",
    "    ),\n",
    "    # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever\n",
    "    query_transform_prompt | model | StrOutputParser() | retriever,\n",
    ").with_config(run_name=\"chat_retriever_chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 19: Finalizing the Conversational Retrieval-Augmented Generation (RAG) Pipeline\n",
    "\n",
    "This step integrates all the components to build the final conversational RAG pipeline, which can handle multi-turn conversations, transform queries, retrieve relevant documents, and generate accurate answers.\n",
    "\n",
    "1. **System Template**:\n",
    "   - The `SYSTEM_TEMPLATE` defines the behavior of the answer generation system:\n",
    "     - Instructs the model to base its answers solely on the provided context.\n",
    "     - Explicitly directs the model to say `\"I don't know\"` if the context lacks relevant information, reducing the risk of hallucinations.\n",
    "\n",
    "2. **Question-Answering Prompt**:\n",
    "   - The `ChatPromptTemplate.from_messages()` creates a structured prompt for the system.\n",
    "   - Components:\n",
    "     - **System Message**: Sets the rules and behavior for answer generation.\n",
    "     - **Messages Placeholder**: Captures the conversational context and query for generating the final response.\n",
    "\n",
    "3. **Document Chain**:\n",
    "   - The `create_stuff_documents_chain()` function combines the generative model (`model`) with the question-answering prompt.\n",
    "   - This chain processes the retrieved context and conversational history to generate a coherent and relevant answer.\n",
    "\n",
    "4. **Conversational Retrieval Chain**:\n",
    "   - The `RunnablePassthrough.assign()` method is used to sequentially integrate:\n",
    "     - **Context Retrieval**:\n",
    "       - The `query_transforming_retriever_chain` retrieves relevant documents based on transformed queries or direct input.\n",
    "     - **Answer Generation**:\n",
    "       - The `document_chain` generates a final response based on the retrieved context and user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system template for generating answers\n",
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "Answer the user's questions based on the below context. \n",
    "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt template for question answering (refer to Step 9 for prompt creation)\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            SYSTEM_TEMPLATE,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),  # Adds conversational context (Step 9)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a document chain for answering user questions (refer to Step 9)\n",
    "document_chain = create_stuff_documents_chain(model, question_answering_prompt)\n",
    "\n",
    "# Build the final conversational retrieval chain\n",
    "# Combine the transformed query retrieval (Step 18) with the document chain (Step 9)\n",
    "conversational_retrieval_chain = RunnablePassthrough.assign(\n",
    "    # Assign the transformed query context to the retrieval chain (refer to Step 18)\n",
    "    context=query_transforming_retriever_chain,\n",
    ").assign(\n",
    "    # Assign the answer generation process to the document chain (refer to Step 9)\n",
    "    answer=document_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 20: Testing the Conversational Retrieval Chain with an Unrelated Query\n",
    "\n",
    "This step tests the robustness of the final conversational retrieval-augmented generation (RAG) pipeline by providing a query unrelated to the available context in the documents.\n",
    "\n",
    "#### Notes:\n",
    "- **Relevance of the Test**:\n",
    "   - Real-world systems often encounter queries beyond their knowledge scope, making this a critical behavior to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_retrieval_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 21: Verifying the Conversational Retrieval Chain in the Target Use Case\n",
    "\n",
    "1. **Test Input**:\n",
    "   - **Initial Query**:\n",
    "     - `\"What is the difference between high and medium protein-based diets?\"`\n",
    "   - **AI Response** (provided in the test input to simulate a prior response):\n",
    "     - Summarizes a study about the effects of high and normal protein diets on body composition and glucose control.\n",
    "   - **Follow-Up Query**:\n",
    "     - `\"Tell me more!\"`—a vague request that relies on the system to infer and retrieve additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_retrieval_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"What is the difference between high and medium protein-based diets?\"),\n",
    "            AIMessage(\n",
    "                content=\"he study found that both high and normal protein diets improved body composition and glucose control in adults with type 2 diabetes. The lack of observed effects of dietary protein and red meat consumption on weight loss and improved cardiometabolic health suggest that achieved weight loss – rather than diet composition – should be the principal target of dietary interventions for T2D management.\"\n",
    "            ),\n",
    "            HumanMessage(content=\"Tell me more!\"),\n",
    "        ],\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 22: Wrapping the Conversational Retrieval Pipeline in a Function\n",
    "\n",
    "This step wraps the entire conversational retrieval-augmented generation (RAG) pipeline into a single reusable function. The function manages the conversation history, tracks the query, and generates answers, ensuring a seamless user experience.\n",
    "\n",
    "It is decorated with `@weave.op()` to log and track the process in **Weights & Biases** (Weave).\n",
    "\n",
    "The `@weave.op()` decorator ensures that each query and response are logged in **Weights & Biases**, enabling tracking, monitoring, and debugging of the conversational process.\n",
    "\n",
    "5. **Example Usage**:\n",
    "   ```python\n",
    "   conversation = {\"messages\": []}\n",
    "   print(await get_answer(\"What is the difference between high and medium protein-based diets?\", conversation))\n",
    "   print(await get_answer(\"Tell me more!\", conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def get_answer(question: str, messages: dict):\n",
    "    \"\"\"\n",
    "    Handles user queries by appending them to the conversation history, \n",
    "    processing the query through the conversational retrieval chain, \n",
    "    and appending the AI's response back to the messages.\n",
    "\n",
    "    Parameters:\n",
    "    - question (str): The user's input question.\n",
    "    - messages (dict): A dictionary containing the conversation history \n",
    "                       with a \"messages\" key holding a list of message objects.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated answer from the system.\n",
    "    \"\"\"\n",
    "    # Add the user's question to the conversation history\n",
    "    messages[\"messages\"].append(HumanMessage(content=question))\n",
    "    \n",
    "    # Process the query through the conversational retrieval chain\n",
    "    answer = conversational_retrieval_chain.invoke(messages)\n",
    "    \n",
    "    # Add the system's response to the conversation history\n",
    "    messages[\"messages\"].append(AIMessage(content=answer[\"answer\"]))\n",
    "    \n",
    "    # Return the generated answer\n",
    "    return answer[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = {\"messages\": []} \n",
    "answer = asyncio.get_event_loop().run_until_complete(get_answer(\"What is the difference between high and medium protein-based diets?\", messages))\n",
    "print(answer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green; font-size: 16px;\">\n",
    "<b>Exercise: Testing the Full Retrieval-Augmented Generation (RAG) Pipeline</b><br>\n",
    "<b>Goal:</b> Evaluate the performance of the complete RAG pipeline by testing it with diverse queries and identifying its strengths and weaknesses.<br><br>\n",
    "\n",
    "<b>Instructions:</b><br>\n",
    "<ul style=\"color:green;\">\n",
    "<li><b>Prepare a Set of Queries:</b></li>\n",
    "<ul style=\"color:green;\">\n",
    "    <li>Create at least five queries that vary in nature, including:</li>\n",
    "    <ul style=\"color:green;\">\n",
    "        <li>A highly specific query.</li>\n",
    "        <li>A vague or open-ended query.</li>\n",
    "        <li>A multi-part or follow-up query.</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "<li><b>Run the Pipeline:</b></li>\n",
    "<ul style=\"color:green;\">\n",
    "    <li>Input each query into the full RAG pipeline.</li>\n",
    "    <li>Observe and document the generated responses, including:</li>\n",
    "    <ul style=\"color:green;\">\n",
    "        <li>The documents retrieved.</li>\n",
    "        <li>The final generated output.</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "<li><b>Analyze Performance:</b></li>\n",
    "<ul style=\"color:green;\">\n",
    "    <li>For each query, analyze:</li>\n",
    "    <ul style=\"color:green;\">\n",
    "        <li><b>Strengths:</b> What aspects of the query were handled well (e.g., relevance, coherence, accuracy)?</li>\n",
    "        <li><b>Weaknesses:</b> What challenges did the pipeline face (e.g., irrelevant documents, poor contextual understanding, incomplete answers)?</li>\n",
    "        <li><b>Patterns:</b> Are there consistent issues or successes across the queries?</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "<li><b>Document Findings:</b></li>\n",
    "<ul style=\"color:green;\">\n",
    "    <li>Checke the following in weave:</li>\n",
    "    <ul style=\"color:green;\">\n",
    "        <li><b>Query type.</b></li>\n",
    "        <li><b>Retrieved documents</b> (relevance and quality).</li>\n",
    "        <li><b>Generated output</b> (clarity and accuracy).</li>\n",
    "        <li><b>Overall evaluation</b> (what worked, what didn’t).</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "<b>Purpose:</b> This exercise helps you critically evaluate the end-to-end functionality of the RAG pipeline, highlighting areas of success and identifying limitations that could guide future improvements.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
